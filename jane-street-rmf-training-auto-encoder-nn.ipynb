{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"},{"sourceId":214524751,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":7.594014,"end_time":"2024-10-10T11:58:36.355301","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-10T11:58:28.761287","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is used to train NN with autoencoder, this is an implementation of https://www.kaggle.com/code/gogo827jz/jane-street-supervised-autoencoder-mlp  \ndataset is constructed by https://www.kaggle.com/code/motono0223/js24-preprocessing-create-lags","metadata":{}},{"cell_type":"markdown","source":"# Training Neural Networks (MLP) with PyTorch Lightning","metadata":{}},{"cell_type":"code","source":"import os\nimport pickle\nimport polars as pl\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-12-29T05:07:41.846144Z","iopub.execute_input":"2024-12-29T05:07:41.846505Z","iopub.status.idle":"2024-12-29T05:07:42.353525Z","shell.execute_reply.started":"2024-12-29T05:07:41.846476Z","shell.execute_reply":"2024-12-29T05:07:42.352685Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"#This block load data. Dataset is different for running locally on kaggle platform and server\nLOCAL_TRAINING = True\nfeature_names = [f\"feature_{i:02d}\" for i in range(79)] + [f\"responder_{idx}_lag_1\" for idx in range(9)]\nlabel_name = 'responder_6'\nweight_name = 'weight'\ntrain_loc = os.path.join(\"/kaggle/input/test_training_dataset.pickle\")\nvalid_loc = os.path.join(\"/kaggle/input/test_validation_dataset.pickle\")\nif LOCAL_TRAINING:\n    df = pl.scan_parquet(\n    f\"/kaggle/input/k/qinyanghe/js24-preprocessing-create-lags/test_training.parquet\"\n).collect().to_pandas()\n    valid = pl.scan_parquet(\n    f\"/kaggle/input/k/qinyanghe/js24-preprocessing-create-lags/test_validation.parquet\"\n).collect().to_pandas()\n    df = pd.concat([df, valid]).reset_index(drop=True)# A trick to boost LB from 0.0045->0.005","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T05:07:42.520870Z","iopub.execute_input":"2024-12-29T05:07:42.521285Z","iopub.status.idle":"2024-12-29T05:07:59.559421Z","shell.execute_reply.started":"2024-12-29T05:07:42.521256Z","shell.execute_reply":"2024-12-29T05:07:59.558494Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"X_train = df[ feature_names ]\ny_train = df[ label_name ]\nw_train = df[ \"weight\" ]\nX_valid = valid[ feature_names ]\ny_valid = valid[ label_name ]\nw_valid = valid[ \"weight\" ]\n\nX_train.shape, y_train.shape, w_train.shape, X_valid.shape, y_valid.shape, w_valid.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T05:07:59.560947Z","iopub.execute_input":"2024-12-29T05:07:59.561183Z","iopub.status.idle":"2024-12-29T05:08:00.503043Z","shell.execute_reply.started":"2024-12-29T05:07:59.561159Z","shell.execute_reply":"2024-12-29T05:08:00.502321Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"((7361640, 88), (7361640,), (7361640,), (338800, 88), (338800,), (338800,))"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"X_train['responder_6_lag_1'][X_train[\"responder_6_lag_1\"].notnull()].","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T04:47:18.380753Z","iopub.execute_input":"2024-12-29T04:47:18.381025Z","iopub.status.idle":"2024-12-29T04:47:18.387116Z","shell.execute_reply.started":"2024-12-29T04:47:18.381000Z","shell.execute_reply":"2024-12-29T04:47:18.385727Z"}},"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    X_train['responder_6_lag_1'][X_train[\"responder_6_lag_1\"].notnull()].\u001b[0m\n\u001b[0m                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (2376126448.py, line 1)","output_type":"error"}],"execution_count":4},{"cell_type":"markdown","source":"# Training Configurations","metadata":{}},{"cell_type":"code","source":"import os\nimport warnings\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom pytorch_lightning import (LightningDataModule, LightningModule, Trainer)\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Timer\nfrom pytorch_lightning.loggers import WandbLogger\nfrom torchvision.transforms.v2 import GaussianNoise\nimport wandb\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\n","metadata":{"execution":{"iopub.status.busy":"2024-12-29T05:08:02.646143Z","iopub.execute_input":"2024-12-29T05:08:02.646464Z","iopub.status.idle":"2024-12-29T05:08:10.035169Z","shell.execute_reply.started":"2024-12-29T05:08:02.646435Z","shell.execute_reply":"2024-12-29T05:08:10.034240Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# PyTorch Data Module Definition","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, df, accelerator):\n        self.features = torch.FloatTensor(df[feature_names].values).to(accelerator)\n        self.labels = torch.FloatTensor(df[label_name].values).to(accelerator)\n        self.weights = torch.FloatTensor(df[weight_name].values).to(accelerator)\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        x = self.features[idx]\n        y = self.labels[idx]\n        w = self.weights[idx]\n        return x, y, w\n\n\nclass DataModule(LightningDataModule):\n    def __init__(self, train_df, batch_size, valid_df=None, accelerator='cpu'):\n        super().__init__()\n        self.df = train_df\n        self.batch_size = batch_size\n        self.dates = self.df['date_id'].unique()\n        self.accelerator = accelerator\n        self.train_dataset = None\n        self.valid_df = None\n        if valid_df is not None:\n            self.valid_df = valid_df\n        self.val_dataset = None\n\n    def setup(self, fold=0, N_fold=5, stage=None):\n        # Split dataset\n        selected_dates = [date for ii, date in enumerate(self.dates) if ii % N_fold != fold]\n        df_train = self.df.loc[self.df['date_id'].isin(selected_dates)]\n        self.train_dataset = CustomDataset(df_train, self.accelerator)\n        if self.valid_df is not None:\n            df_valid = self.valid_df\n            self.val_dataset = CustomDataset(df_valid, self.accelerator)\n\n    def train_dataloader(self, n_workers=0):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=n_workers)\n\n    def val_dataloader(self, n_workers=0):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=n_workers)\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-12-29T05:08:10.036501Z","iopub.execute_input":"2024-12-29T05:08:10.036796Z","iopub.status.idle":"2024-12-29T05:08:10.046266Z","shell.execute_reply.started":"2024-12-29T05:08:10.036769Z","shell.execute_reply":"2024-12-29T05:08:10.045255Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Autoencoder NN Model Definition","metadata":{}},{"cell_type":"code","source":"# Custom R2 metric for validation\ndef r2_val(y_true, y_pred, sample_weight):\n    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38)\n    return r2\n\n\nclass AE_NN(LightningModule):\n    def __init__(self, input_dim,ae_dims, nn_hidden_dims, ae_dropouts, nn_dropouts, noise_std,loss_weights, lr, weight_decay):\n        super().__init__()\n        self.save_hyperparameters()\n        in_dim = input_dim\n        self.ae_1 = nn.BatchNorm1d(in_dim)\n        #gaussian noise at this step\n        self.ae_2 = nn.Linear(in_dim,ae_dims[0])\n        self.ae_3 = nn.BatchNorm1d(ae_dims[0])\n        self.ae_4 = nn.SiLU()\n        #SiLU as the final output of encoder?\n        self.ae_5 = nn.Dropout(ae_dropouts[0])\n        self.ae_6 = nn.Linear(ae_dims[0],input_dim)\n        #output of ae_6 is recovered features\n        #the following is for autoencoder regression\n        self.ae_7 = nn.Linear(input_dim,ae_dims[1])\n        self.ae_8 = nn.BatchNorm1d(ae_dims[1])\n        self.ae_9 = nn.SiLU()\n        self.ae_10 = nn.Dropout(ae_dropouts[1])\n        self.ae_11 = nn.Linear(ae_dims[1],1)\n        self.MLP_layers = nn.ModuleList()\n        self.input_dim = input_dim\n        in_dim = ae_dims[0] + self.input_dim\n        for i, hidden_dim in enumerate(nn_hidden_dims):\n            self.MLP_layers.append(nn.BatchNorm1d(in_dim))\n            if i > 0:\n                self.MLP_layers.append(nn.SiLU())\n            if i < len(nn_dropouts):\n                self.MLP_layers.append(nn.Dropout(nn_dropouts[i]))\n            self.MLP_layers.append(nn.Linear(in_dim, hidden_dim))\n            # layers.append(nn.ReLU())\n            in_dim = hidden_dim\n        self.MLP_layers.append(nn.Linear(in_dim, 1))  # 输出层\n        #self.MLP_layers.append(nn.Tanh())\n        #why output activation is Tanh?\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.validation_step_outputs = []\n        self.noise_std = noise_std\n        self.loss_weights = loss_weights\n        self.val_r_square_history = []\n        \n    def forward(self,inp):\n        x = self.ae_1(inp)\n        x = x+ self.noise_std * torch.randn(self.input_dim).to(inp.device)\n        x = self.ae_2(x)\n        x = self.ae_3(x)\n        encoder = self.ae_4(x)\n        x = self.ae_5(encoder)\n        decoder = self.ae_6(x)\n        x = self.ae_7(decoder)\n        x = self.ae_8(x)\n        x = self.ae_9(x)\n        x = self.ae_10(x)\n        out_ae = self.ae_11(x)\n\n        x = torch.cat((encoder,inp),1)\n        for layer in self.MLP_layers:\n            x = layer(x)\n        return decoder, out_ae, x\n    \n    def training_step(self,batch):\n        x,y,w = batch\n        decoder_hat, out_ae_hat,y_hat = self(x)\n        decoder_loss = F.mse_loss(decoder_hat,x)  \n        y = y.view(-1,1)\n        out_ae_loss = F.mse_loss(out_ae_hat,y, reduction = 'none') * w\n        y_loss = F.mse_loss(y_hat,y, reduction = 'none') * w\n        decoder_loss = decoder_loss.mean()\n        out_ae_loss = out_ae_loss.mean()\n        y_loss = y_loss.mean()\n        \n        # y_np = y.clone().detach().cpu().numpy()\n        # y_hat_np = y_hat.clone().detach().cpu().numpy()\n        # w_np = w.clone().detach().cpu().numpy()\n        # val_r_square = r2_val(y_np, y_hat_np,\n        #                       w_np)\n        r_square = self.r2_val_torch(y,y_hat, w)\n        self.log('decoder_loss', decoder_loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n        self.log('out_ae_loss', out_ae_loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n        self.log('y_loss', y_loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n        self.log('r_square', r_square, on_step=False, on_epoch=True, batch_size=x.size(0))\n        w_1,w_2,w_3 = self.loss_weights[0],self.loss_weights[1],self.loss_weights[2]\n        loss = w_1 * decoder_loss + w_2 * out_ae_loss + w_3 * y_loss\n        return loss\n        \n    def validation_step(self, batch, batch_idx):\n        if self.trainer.sanity_checking:\n            return  # Skip during sanity check\n        x, y, w = batch\n        decoder_hat, out_ae_hat, y_hat = self(x)\n        \n        # Compute validation losses\n        decoder_loss = F.mse_loss(decoder_hat, x)\n        y = y.view(-1, 1)\n        out_ae_loss = F.mse_loss(out_ae_hat, y, reduction='none') * w\n        y_loss = F.mse_loss(y_hat, y, reduction='none') * w\n        \n        # Compute r_square for this batch\n        r_square = self.r2_val_torch(y, y_hat, w)\n        \n        # Log metrics\n        self.log('val_decoder_loss', decoder_loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n        self.log('val_out_ae_loss', out_ae_loss.mean(), on_step=False, on_epoch=True, batch_size=x.size(0))\n        self.log('val_y_loss', y_loss.mean(), on_step=False, on_epoch=True, batch_size=x.size(0))\n        self.log('val_r_square', r_square, on_step=False, on_epoch=True, batch_size=x.size(0))\n        \n        # Store only necessary tensors with detached copies\n        self.validation_step_outputs.append((\n            y_hat.detach(),  # Detach to free computational graph\n            y.detach(),      # Detach to free computational graph\n            w.detach()       # Detach to free computational graph\n        ))\n\n    def on_validation_epoch_end(self):\n        if not self.trainer.sanity_checking:\n            try:\n                # # Efficient concatenation with proper memory management\n                # y_true = torch.cat([x[1] for x in self.validation_step_outputs])\n                # y_pred = torch.cat([x[0] for x in self.validation_step_outputs])\n                # weights = torch.cat([x[2] for x in self.validation_step_outputs])\n                \n                # # Compute epoch-level r_square\n                # epoch_r_square = self.r2_val_torch(y_true, y_pred, weights)\n                \n                # # Log and print metrics\n                # self.log(\"val_r_square_epoch\", epoch_r_square, prog_bar=True)\n                # self.val_r_square_history.append(epoch_r_square.item())\n                # print(f\"\\nValidation R² Score (Epoch {self.trainer.current_epoch}): {epoch_r_square:.4f}\")\n                current_epoch = self.trainer.current_epoch\n                val_decoder_loss = trainer.logged_metrics['val_decoder_loss']\n                val_out_ae_loss = trainer.logged_metrics['val_out_ae_loss']\n                val_y_loss = trainer.logged_metrics['val_y_loss']\n                val_r_square = trainer.logged_metrics['val_r_square']\n                self.val_r_square_history.append(val_r_square.item())\n                formatted_metrics = {'val_decoder_loss':f\"{val_decoder_loss:.5f}\",'val_out_ae_loss':f\"{val_out_ae_loss:.5f}\",\n                            'val_y_loss':f\"{val_y_loss:.5f}\",'val_r_square':f\"{val_r_square:.5f}\"}\n                print(f\"Epoch {current_epoch}: {formatted_metrics}\")\n                if current_epoch!= 0 and current_epoch % 10 == 0:  # Plot every 5 epochs\n                    self.plot_r_square_history()\n                \n            finally:\n                # Ensure cleanup happens even if there's an error\n                self.validation_step_outputs.clear()\n\n        \n    def r2_val_torch(self,y_true: torch.Tensor, y_pred: torch.Tensor, sample_weight: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Calculate weighted R² score using PyTorch tensors.\n        \n        Args:\n            y_true: Ground truth values\n            y_pred: Predicted values\n            sample_weight: Weights for each sample\n            \n        Returns:\n            Weighted R² score\n        \"\"\"\n        # Ensure all inputs are tensors and on the same device\n        if not torch.is_tensor(sample_weight):\n            sample_weight = torch.tensor(sample_weight, device=y_true.device)\n        \n        # Calculate weighted MSE (numerator)\n        weighted_mse = torch.sum(sample_weight * (y_pred - y_true) ** 2) / torch.sum(sample_weight)\n        \n        # Calculate weighted variance (denominator)\n        weighted_var = torch.sum(sample_weight * y_true ** 2) / torch.sum(sample_weight) + 1e-38\n        \n        # Calculate R²\n        r2 = 1 - weighted_mse / weighted_var\n        \n        return r2\n    def plot_r_square_history(self):\n        \"\"\"\n        Plot the history of validation R² scores\n        \"\"\"\n        try:\n            import matplotlib.pyplot as plt\n            \n            plt.figure(figsize=(10, 6))\n            plt.plot(self.val_r_square_history, '-o')\n            plt.title('Validation R² Score History')\n            plt.xlabel('Epoch')\n            plt.ylabel('R² Score')\n            plt.grid(True)\n            plt.savefig(f'r2_history_epoch_{self.trainer.current_epoch}.png')\n            plt.close()\n        except ImportError:\n            print(\"Matplotlib not available for plotting\")\n            \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5,\n                                                               verbose=True)\n        return {\n            'optimizer': optimizer,\n            'lr_scheduler': {\n                'scheduler': scheduler,\n                'monitor': 'y_loss',\n            }\n        }\n\n    def on_train_epoch_end(self):\n        if self.trainer.sanity_checking:\n            return\n        epoch = self.trainer.current_epoch\n        w_1,w_2,w_3 = self.loss_weights[0],self.loss_weights[1],self.loss_weights[2]\n        decoder_loss = trainer.logged_metrics['decoder_loss']\n        out_ae_loss = trainer.logged_metrics['out_ae_loss']\n        r_square = trainer.logged_metrics['r_square']\n        y_loss = trainer.logged_metrics['y_loss']\n        # metrics = {k: v.item() if isinstance(v, torch.Tensor) else v for k, v in self.trainer.logged_metrics.items()}\n        # formatted_metrics = {k: f\"{v:.5f}\" for k, v in metrics.items()}\n        formatted_metrics = {'decoder_loss':f\"{decoder_loss:.5f}\",'out_ae_loss':f\"{out_ae_loss:.5f}\",\n                            'y_loss':f\"{y_loss:.5f}\",'r_square':f\"{r_square:.5f}\"}\n        print(f\"Epoch {epoch}: {formatted_metrics}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T05:31:59.222490Z","iopub.execute_input":"2024-12-29T05:31:59.222931Z","iopub.status.idle":"2024-12-29T05:31:59.251042Z","shell.execute_reply.started":"2024-12-29T05:31:59.222898Z","shell.execute_reply":"2024-12-29T05:31:59.250091Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# Create PyTorch Data Module","metadata":{}},{"cell_type":"code","source":"class custom_args():\n    def __init__(self):\n        self.usegpu = True\n        self.gpuid = 0\n        self.seed = 42\n        self.model = 'nn'\n        self.use_wandb = False\n        self.project = 'js-xs-nn-with-lags'\n        self.dname = \"./input_df/\"\n        self.loader_workers = 4\n        self.bs = 8192\n        self.lr = 1e-3\n        self.weight_decay = 5e-4\n        self.ae_dropouts = [0.1,0.1]\n        self.dropouts = [0.1, 0.1]\n        self.ae_dims = [40,120]\n        self.n_hidden = [512, 512, 256]\n        self.noise_std = 1\n        self.loss_weights = [0.1,1,1]\n        self.patience = 10\n        self.max_epochs = 100\n        self.N_fold = 5\n\n\n\nargs = custom_args()\n\n# checking device\ndevice = torch.device(f'cuda:{args.gpuid}' if torch.cuda.is_available() and args.usegpu else 'cpu')\naccelerator = 'gpu' if torch.cuda.is_available() and args.usegpu else 'cpu'\nloader_device = 'cpu'\n\n\n# Initialize Data Module\n\ndf[feature_names] = df[feature_names].fillna(method = 'ffill').fillna(0)\nvalid[feature_names] = valid[feature_names].fillna(method = 'ffill').fillna(0)\ndata_module = DataModule(df, batch_size=args.bs, valid_df=valid, accelerator=loader_device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T05:22:44.066003Z","iopub.execute_input":"2024-12-29T05:22:44.066839Z","iopub.status.idle":"2024-12-29T05:22:44.111731Z","shell.execute_reply.started":"2024-12-29T05:22:44.066801Z","shell.execute_reply":"2024-12-29T05:22:44.110050Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 36\u001b[0m\n\u001b[1;32m     31\u001b[0m loader_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Initialize Data Module\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m df[feature_names] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[feature_names]\u001b[38;5;241m.\u001b[39mfillna(method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mffill\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     37\u001b[0m valid[feature_names] \u001b[38;5;241m=\u001b[39m valid[feature_names]\u001b[38;5;241m.\u001b[39mfillna(method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mffill\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     38\u001b[0m data_module \u001b[38;5;241m=\u001b[39m DataModule(df, batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbs, valid_df\u001b[38;5;241m=\u001b[39mvalid, accelerator\u001b[38;5;241m=\u001b[39mloader_device)\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"],"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error"}],"execution_count":15},{"cell_type":"markdown","source":"# Create Model and Training","metadata":{}},{"cell_type":"code","source":"import gc\ntry:\n    del df\nexcept NameError:\n    pass \ngc.collect()\npl.seed_everything(args.seed)\nfor fold in range(args.N_fold):\n    data_module.setup(fold, args.N_fold)\n    # Obtain input dimension\n    input_dim = data_module.train_dataset.features.shape[1]\n    # Initialize Model\n    model = AE_NN(\n        input_dim=input_dim,\n        ae_dims = args.ae_dims,\n        nn_hidden_dims=args.n_hidden,\n        ae_dropouts = args.ae_dropouts,\n        nn_dropouts=args.dropouts,\n        noise_std = args.noise_std,\n        loss_weights = args.loss_weights,\n        lr=args.lr,\n        weight_decay=args.weight_decay\n    )\n    # Initialize Logger\n    if args.use_wandb:\n        wandb_run = wandb.init(project=args.project, config=vars(args), reinit=True)\n        logger = WandbLogger(experiment=wandb_run)\n    else:\n        logger = None\n    # Initialize Callbacks\n    early_stopping = EarlyStopping('y_loss', patience=args.patience, mode='min', verbose=False)\n    checkpoint_callback = ModelCheckpoint(monitor='y_loss', mode='min', save_top_k=1, verbose=False, filename=f\"./models/nn_{fold}.model\") \n    timer = Timer()\n    # Initialize Trainer\n    trainer = Trainer(\n        max_epochs=args.max_epochs,\n        accelerator=accelerator,\n        devices=[args.gpuid] if args.usegpu else None,\n        logger=logger,\n        callbacks=[early_stopping, checkpoint_callback, timer],\n        enable_progress_bar=True,\n        num_sanity_val_steps=0\n    )\n    # Start Training\n    trainer.fit(model, data_module.train_dataloader(args.loader_workers), data_module.val_dataloader(args.loader_workers))\n    # You can find trained best model in your local path\n    print(f'Fold-{fold} Training completed in {timer.time_elapsed(\"train\"):.2f}s')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T05:32:03.322051Z","iopub.execute_input":"2024-12-29T05:32:03.322851Z","iopub.status.idle":"2024-12-29T05:40:43.796890Z","shell.execute_reply.started":"2024-12-29T05:32:03.322814Z","shell.execute_reply":"2024-12-29T05:40:43.794229Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cda6d08194d45cb8af54bb374516aa9"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 0: {'val_decoder_loss': '296.75375', 'val_out_ae_loss': '1.21437', 'val_y_loss': '1.29764', 'val_r_square': '-0.07140'}\nEpoch 0: {'decoder_loss': '494.49680', 'out_ae_loss': '1.50280', 'y_loss': '1.48437', 'r_square': '0.00958'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1: {'val_decoder_loss': '125.58650', 'val_out_ae_loss': '1.21347', 'val_y_loss': '3.11844', 'val_r_square': '-1.08761'}\nEpoch 1: {'decoder_loss': '158.46252', 'out_ae_loss': '1.49631', 'y_loss': '1.44562', 'r_square': '0.03529'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2: {'val_decoder_loss': '124.58168', 'val_out_ae_loss': '1.22663', 'val_y_loss': '5.28300', 'val_r_square': '-3.32389'}\nEpoch 2: {'decoder_loss': '20.99688', 'out_ae_loss': '1.49445', 'y_loss': '1.43332', 'r_square': '0.04347'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3: {'val_decoder_loss': '123.97749', 'val_out_ae_loss': '1.24730', 'val_y_loss': '30.71189', 'val_r_square': '-17.51278'}\nEpoch 3: {'decoder_loss': '5.08809', 'out_ae_loss': '1.49281', 'y_loss': '1.42359', 'r_square': '0.04995'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4: {'val_decoder_loss': '151.54704', 'val_out_ae_loss': '1.29259', 'val_y_loss': '11.21717', 'val_r_square': '-12.45285'}\nEpoch 4: {'decoder_loss': '4.22250', 'out_ae_loss': '1.49194', 'y_loss': '1.41813', 'r_square': '0.05362'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5: {'val_decoder_loss': '136.65465', 'val_out_ae_loss': '2.64616', 'val_y_loss': '64.88599', 'val_r_square': '-53.48793'}\nEpoch 5: {'decoder_loss': '4.04589', 'out_ae_loss': '1.48910', 'y_loss': '1.41483', 'r_square': '0.05582'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 6: {'val_decoder_loss': '103.50160', 'val_out_ae_loss': '5.11906', 'val_y_loss': '50.50990', 'val_r_square': '-40.62472'}\nEpoch 6: {'decoder_loss': '3.90974', 'out_ae_loss': '1.48367', 'y_loss': '1.41235', 'r_square': '0.05746'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7: {'val_decoder_loss': '122.42485', 'val_out_ae_loss': '6.86431', 'val_y_loss': '66.36824', 'val_r_square': '-66.17456'}\nEpoch 7: {'decoder_loss': '3.79798', 'out_ae_loss': '1.47956', 'y_loss': '1.41017', 'r_square': '0.05893'}\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x781259593e80>>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n    def _clean_thread_parent_frames(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:212\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    211\u001b[0m dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m batch, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py:133\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py:60\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py:78\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     out[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1293\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1293\u001b[0m     success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/reductions.py:496\u001b[0m, in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrebuild_storage_fd\u001b[39m(\u001b[38;5;28mcls\u001b[39m, df, size):\n\u001b[0;32m--> 496\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py:57\u001b[0m, in \u001b[0;36mDupFd.detach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Get the fd.  This should only be called once.'''\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_resource_sharer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduction\u001b[38;5;241m.\u001b[39mrecv_handle(conn)\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py:86\u001b[0m, in \u001b[0;36m_ResourceSharer.get_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     85\u001b[0m address, key \u001b[38;5;241m=\u001b[39m ident\n\u001b[0;32m---> 86\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m c\u001b[38;5;241m.\u001b[39msend((key, os\u001b[38;5;241m.\u001b[39mgetpid()))\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:508\u001b[0m, in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m authkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 508\u001b[0m     \u001b[43manswer_challenge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m     deliver_challenge(c, authkey)\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:752\u001b[0m, in \u001b[0;36manswer_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    751\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthkey must be bytes, not \u001b[39m\u001b[38;5;132;01m{0!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(authkey)))\n\u001b[0;32m--> 752\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m         \u001b[38;5;66;03m# reject large message\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m message[:\u001b[38;5;28mlen\u001b[39m(CHALLENGE)] \u001b[38;5;241m==\u001b[39m CHALLENGE, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage = \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m message\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:216\u001b[0m, in \u001b[0;36m_ConnectionBase.recv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative maxlength\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 216\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 44\u001b[0m\n\u001b[1;32m     35\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     36\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_epochs,\n\u001b[1;32m     37\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39maccelerator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     enable_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Start Training\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader_workers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader_workers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# You can find trained best model in your local path\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFold-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Training completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimer\u001b[38;5;241m.\u001b[39mtime_elapsed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n","\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"],"ename":"NameError","evalue":"name 'exit' is not defined","output_type":"error"},{"name":"stderr","text":"KeyboardInterrupt: \nException ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x781259593e80>>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n    def _clean_thread_parent_frames(\nKeyboardInterrupt: \n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"data_module.train_dataset.features.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T03:36:15.057418Z","iopub.execute_input":"2024-12-29T03:36:15.057888Z","iopub.status.idle":"2024-12-29T03:36:15.065747Z","shell.execute_reply.started":"2024-12-29T03:36:15.057820Z","shell.execute_reply":"2024-12-29T03:36:15.064819Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"torch.Size([5882536, 88])"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}